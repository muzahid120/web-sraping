{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies: 10000\n",
      "Number of ratings: 10000\n",
      "Number of reviews: 10000\n",
      "Number of salaries: 10000\n",
      "Number of interviews: 10000\n",
      "Number of jobs: 10000\n",
      "Number of industries: 10000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize lists to store data\n",
    "company_names = []\n",
    "ratings = []\n",
    "reviews = []\n",
    "salaries = []\n",
    "interviews = []\n",
    "jobs = []\n",
    "industries = []\n",
    "\n",
    "# Iterate through 400 pages\n",
    "for page_num in range(1, 504):\n",
    "    # Define the URL for each page\n",
    "    url = f'https://www.ambitionbox.com/list-of-companies?campaign=desktop_nav&page={page_num}'\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all company cards\n",
    "        company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "        \n",
    "        # Loop through each company card and extract information\n",
    "        for card in company_cards:\n",
    "            # Extract company name\n",
    "            company_name = card.find('h2', class_='companyCardWrapper__companyName').text.strip()\n",
    "            company_names.append(company_name)\n",
    "\n",
    "            # Extract company rating\n",
    "            company_rating = card.find('span', class_='companyCardWrapper__companyRatingValue').text.strip()\n",
    "            ratings.append(company_rating)\n",
    "\n",
    "            # Extract number of reviews\n",
    "            reviews_count_elem = card.find('a', href='https://www.ambitionbox.com/reviews/tcs-reviews')\n",
    "            if reviews_count_elem:\n",
    "                reviews_count = reviews_count_elem.text.strip()\n",
    "                reviews.append(reviews_count)\n",
    "            else:\n",
    "                reviews.append('N/A')\n",
    "\n",
    "            # Extract number of salaries\n",
    "            salaries_count_elem = card.find('a', href='https://www.ambitionbox.com/salaries/tcs-salaries')\n",
    "            if salaries_count_elem:\n",
    "                salaries_count = salaries_count_elem.text.strip()\n",
    "                salaries.append(salaries_count)\n",
    "            else:\n",
    "                salaries.append('N/A')\n",
    "\n",
    "            # Extract number of interviews\n",
    "            interviews_count_elem = card.find('a', href='https://www.ambitionbox.com/interviews/tcs-interview-questions')\n",
    "            if interviews_count_elem:\n",
    "                interviews_count = interviews_count_elem.text.strip()\n",
    "                interviews.append(interviews_count)\n",
    "            else:\n",
    "                interviews.append('N/A')\n",
    "\n",
    "            # Extract number of jobs\n",
    "            jobs_count_elem = card.find('a', href='https://www.ambitionbox.com/jobs/tcs-jobs')\n",
    "            if jobs_count_elem:\n",
    "                jobs_count = jobs_count_elem.text.strip()\n",
    "                jobs.append(jobs_count)\n",
    "            else:\n",
    "                jobs.append('N/A')\n",
    "\n",
    "            # Extract industry details\n",
    "            industry_details = card.find('span', class_='companyCardWrapper__interLinking').text.strip()\n",
    "            industries.append(industry_details)\n",
    "\n",
    "# Print the length of each list to verify the data\n",
    "print(f\"Number of companies: {len(company_names)}\")\n",
    "print(f\"Number of ratings: {len(ratings)}\")\n",
    "print(f\"Number of reviews: {len(reviews)}\")\n",
    "print(f\"Number of salaries: {len(salaries)}\")\n",
    "print(f\"Number of interviews: {len(interviews)}\")\n",
    "print(f\"Number of jobs: {len(jobs)}\")\n",
    "print(f\"Number of industries: {len(industries)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['926 Jobs',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " 'N/A',\n",
       " ...]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with the extracted data\n",
    "data = {\n",
    "    'Company Name': company_names,\n",
    "    'Rating': ratings,\n",
    "    'Reviews': reviews,\n",
    "    'Salaries': salaries,\n",
    "    'Interviews': interviews,\n",
    "    'Jobs': jobs,\n",
    "    'Industry Details': industries\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Salaries</th>\n",
       "      <th>Interviews</th>\n",
       "      <th>Jobs</th>\n",
       "      <th>Industry Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCS</td>\n",
       "      <td>3.8</td>\n",
       "      <td>74.1k Reviews</td>\n",
       "      <td>848.8k Salaries</td>\n",
       "      <td>6.6k Interviews</td>\n",
       "      <td>926 Jobs</td>\n",
       "      <td>IT Services &amp; Consulting | 1 Lakh+ Employees |...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>IT Services &amp; Consulting | 1 Lakh+ Employees |...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cognizant</td>\n",
       "      <td>3.9</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>IT Services &amp; Consulting | 1 Lakh+ Employees |...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>3.8</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>IT Services &amp; Consulting | 1 Lakh+ Employees |...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capgemini</td>\n",
       "      <td>3.9</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>IT Services &amp; Consulting | 1 Lakh+ Employees |...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company Name Rating        Reviews         Salaries       Interviews  \\\n",
       "0          TCS    3.8  74.1k Reviews  848.8k Salaries  6.6k Interviews   \n",
       "1    Accenture    4.0            N/A              N/A              N/A   \n",
       "2    Cognizant    3.9            N/A              N/A              N/A   \n",
       "3        Wipro    3.8            N/A              N/A              N/A   \n",
       "4    Capgemini    3.9            N/A              N/A              N/A   \n",
       "\n",
       "       Jobs                                   Industry Details  \n",
       "0  926 Jobs  IT Services & Consulting | 1 Lakh+ Employees |...  \n",
       "1       N/A  IT Services & Consulting | 1 Lakh+ Employees |...  \n",
       "2       N/A  IT Services & Consulting | 1 Lakh+ Employees |...  \n",
       "3       N/A  IT Services & Consulting | 1 Lakh+ Employees |...  \n",
       "4       N/A  IT Services & Consulting | 1 Lakh+ Employees |...  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Company Name Rating        Reviews         Salaries  \\\n",
      "0                        TCS    3.8  74.1k Reviews  848.8k Salaries   \n",
      "1                  Accenture    4.0            N/A              N/A   \n",
      "2                  Cognizant    3.9            N/A              N/A   \n",
      "3                      Wipro    3.8            N/A              N/A   \n",
      "4                  Capgemini    3.9            N/A              N/A   \n",
      "...                      ...    ...            ...              ...   \n",
      "9995                   INIFD    3.4            N/A              N/A   \n",
      "9996            Black Turtle    3.6            N/A              N/A   \n",
      "9997                     HTC    3.2            N/A              N/A   \n",
      "9998  Thakral Services India    3.5            N/A              N/A   \n",
      "9999   Mobulous Technologies    4.6            N/A              N/A   \n",
      "\n",
      "           Interviews      Jobs  \\\n",
      "0     6.6k Interviews  926 Jobs   \n",
      "1                 N/A       N/A   \n",
      "2                 N/A       N/A   \n",
      "3                 N/A       N/A   \n",
      "4                 N/A       N/A   \n",
      "...               ...       ...   \n",
      "9995              N/A       N/A   \n",
      "9996              N/A       N/A   \n",
      "9997              N/A       N/A   \n",
      "9998              N/A       N/A   \n",
      "9999              N/A       N/A   \n",
      "\n",
      "                                       Industry Details  \n",
      "0     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "1     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "2     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "3     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "4     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "...                                                 ...  \n",
      "9995  Education & Training | 51-200 Employees (Globa...  \n",
      "9996  Recruitment | 201-500 Employees | 7 years old ...  \n",
      "9997  Consumer Electronics & Appliances | 1-10 Emplo...  \n",
      "9998  Law Enforcement & Security | 501-1k Employees ...  \n",
      "9999  IT Services & Consulting | 51-200 Employees | ...  \n",
      "\n",
      "[10000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Define the total number of pages to scrape\n",
    "total_pages = 503\n",
    "\n",
    "# Initialize lists to store data\n",
    "all_company_names = []\n",
    "all_ratings = []\n",
    "all_reviews = []\n",
    "all_salaries = []\n",
    "all_interviews = []\n",
    "all_jobs = []\n",
    "all_industries = []\n",
    "\n",
    "# Iterate through the pages\n",
    "for page_num in range(1, total_pages + 1):\n",
    "    # Construct the URL for the current page\n",
    "    url = f'https://www.ambitionbox.com/list-of-companies?campaign=desktop_nav&page={page_num}'\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all company cards\n",
    "        company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "        \n",
    "        # Loop through each company card and extract information\n",
    "        for card in company_cards:\n",
    "            # Extract company name\n",
    "            company_name = card.find('h2', class_='companyCardWrapper__companyName').text.strip()\n",
    "            all_company_names.append(company_name)\n",
    "\n",
    "            # Extract company rating\n",
    "            company_rating = card.find('span', class_='companyCardWrapper__companyRatingValue').text.strip()\n",
    "            all_ratings.append(company_rating)\n",
    "\n",
    "            # Extract number of reviews\n",
    "            reviews_count_elem = card.find('a', href='https://www.ambitionbox.com/reviews/tcs-reviews')\n",
    "            reviews_count = reviews_count_elem.text.strip() if reviews_count_elem else 'N/A'\n",
    "            all_reviews.append(reviews_count)\n",
    "\n",
    "            # Extract number of salaries\n",
    "            salaries_count_elem = card.find('a', href='https://www.ambitionbox.com/salaries/tcs-salaries')\n",
    "            salaries_count = salaries_count_elem.text.strip() if salaries_count_elem else 'N/A'\n",
    "            all_salaries.append(salaries_count)\n",
    "\n",
    "            # Extract number of interviews\n",
    "            interviews_count_elem = card.find('a', href='https://www.ambitionbox.com/interviews/tcs-interview-questions')\n",
    "            interviews_count = interviews_count_elem.text.strip() if interviews_count_elem else 'N/A'\n",
    "            all_interviews.append(interviews_count)\n",
    "\n",
    "            # Extract number of jobs\n",
    "            jobs_count_elem = card.find('a', href='https://www.ambitionbox.com/jobs/tcs-jobs')\n",
    "            jobs_count = jobs_count_elem.text.strip() if jobs_count_elem else 'N/A'\n",
    "            all_jobs.append(jobs_count)\n",
    "\n",
    "            # Extract industry details\n",
    "            industry_details = card.find('span', class_='companyCardWrapper__interLinking').text.strip()\n",
    "            all_industries.append(industry_details)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Company Name': all_company_names,\n",
    "    'Rating': all_ratings,\n",
    "    'Reviews': all_reviews,\n",
    "    'Salaries': all_salaries,\n",
    "    'Interviews': all_interviews,\n",
    "    'Jobs': all_jobs,\n",
    "    'Industry Details': all_industries\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Company Name Rating        Reviews         Salaries  \\\n",
      "0                        TCS    3.8  74.1k Reviews  848.8k Salaries   \n",
      "1                  Accenture    4.0            N/A              N/A   \n",
      "2                  Cognizant    3.9            N/A              N/A   \n",
      "3                      Wipro    3.8            N/A              N/A   \n",
      "4                  Capgemini    3.9            N/A              N/A   \n",
      "...                      ...    ...            ...              ...   \n",
      "9995                   INIFD    3.4            N/A              N/A   \n",
      "9996            Black Turtle    3.6            N/A              N/A   \n",
      "9997                     HTC    3.2            N/A              N/A   \n",
      "9998  Thakral Services India    3.5            N/A              N/A   \n",
      "9999   Mobulous Technologies    4.6            N/A              N/A   \n",
      "\n",
      "           Interviews      Jobs  \\\n",
      "0     6.6k Interviews  925 Jobs   \n",
      "1                 N/A       N/A   \n",
      "2                 N/A       N/A   \n",
      "3                 N/A       N/A   \n",
      "4                 N/A       N/A   \n",
      "...               ...       ...   \n",
      "9995              N/A       N/A   \n",
      "9996              N/A       N/A   \n",
      "9997              N/A       N/A   \n",
      "9998              N/A       N/A   \n",
      "9999              N/A       N/A   \n",
      "\n",
      "                                       Industry Details  \n",
      "0     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "1     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "2     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "3     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "4     IT Services & Consulting | 1 Lakh+ Employees |...  \n",
      "...                                                 ...  \n",
      "9995  Education & Training | 51-200 Employees (Globa...  \n",
      "9996  Recruitment | 201-500 Employees | 7 years old ...  \n",
      "9997  Consumer Electronics & Appliances | 1-10 Emplo...  \n",
      "9998  Law Enforcement & Security | 501-1k Employees ...  \n",
      "9999  IT Services & Consulting | 51-200 Employees | ...  \n",
      "\n",
      "[10000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Define the total number of pages to scrape\n",
    "total_pages = 503\n",
    "\n",
    "# Initialize lists to store data\n",
    "all_company_names = []\n",
    "all_ratings = []\n",
    "all_reviews = []\n",
    "all_salaries = []\n",
    "all_interviews = []\n",
    "all_jobs = []\n",
    "all_industries = []\n",
    "\n",
    "# Iterate through the pages\n",
    "for page_num in range(1, total_pages + 1):\n",
    "    # Construct the URL for the current page\n",
    "    url = f'https://www.ambitionbox.com/list-of-companies?campaign=desktop_nav&page={page_num}'\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all company cards\n",
    "        company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "        \n",
    "        # Loop through each company card and extract information\n",
    "        for card in company_cards:\n",
    "            # Extract company name\n",
    "            company_name = card.find('h2', class_='companyCardWrapper__companyName').text.strip()\n",
    "            all_company_names.append(company_name)\n",
    "\n",
    "            # Extract company rating\n",
    "            company_rating = card.find('span', class_='companyCardWrapper__companyRatingValue').text.strip()\n",
    "            all_ratings.append(company_rating)\n",
    "\n",
    "            # Extract number of reviews\n",
    "            try:\n",
    "                reviews_count_elem = card.find('a', href='https://www.ambitionbox.com/reviews/tcs-reviews')\n",
    "                if reviews_count_elem:\n",
    "                    reviews_count = reviews_count_elem.text.strip()\n",
    "                else:\n",
    "                    reviews_count = 'N/A'\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting reviews count: {e}\")\n",
    "                reviews_count = 'N/A'\n",
    "            all_reviews.append(reviews_count)\n",
    "\n",
    "            # Extract number of salaries\n",
    "            try:\n",
    "                salaries_count_elem = card.find('a', href='https://www.ambitionbox.com/salaries/tcs-salaries')\n",
    "                if salaries_count_elem:\n",
    "                    salaries_count = salaries_count_elem.text.strip()\n",
    "                else:\n",
    "                    salaries_count = 'N/A'\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting salaries count: {e}\")\n",
    "                salaries_count = 'N/A'\n",
    "            all_salaries.append(salaries_count)\n",
    "\n",
    "            # Extract number of interviews\n",
    "            try:\n",
    "                interviews_count_elem = card.find('a', href='https://www.ambitionbox.com/interviews/tcs-interview-questions')\n",
    "                if interviews_count_elem:\n",
    "                    interviews_count = interviews_count_elem.text.strip()\n",
    "                else:\n",
    "                    interviews_count = 'N/A'\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting interviews count: {e}\")\n",
    "                interviews_count = 'N/A'\n",
    "            all_interviews.append(interviews_count)\n",
    "\n",
    "            # Extract number of jobs\n",
    "            try:\n",
    "                jobs_count_elem = card.find('a', href='https://www.ambitionbox.com/jobs/tcs-jobs')\n",
    "                if jobs_count_elem:\n",
    "                    jobs_count = jobs_count_elem.text.strip()\n",
    "                else:\n",
    "                    jobs_count = 'N/A'\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting jobs count: {e}\")\n",
    "                jobs_count = 'N/A'\n",
    "            all_jobs.append(jobs_count)\n",
    "\n",
    "            # Extract industry details\n",
    "            try:\n",
    "                industry_details = card.find('span', class_='companyCardWrapper__interLinking').text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting industry details: {e}\")\n",
    "                industry_details = 'N/A'\n",
    "            all_industries.append(industry_details)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Company Name': all_company_names,\n",
    "    'Rating': all_ratings,\n",
    "    'Reviews': all_reviews,\n",
    "    'Salaries': all_salaries,\n",
    "    'Interviews': all_interviews,\n",
    "    'Jobs': all_jobs,\n",
    "    'Industry Details': all_industries\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Company Name Rating        Reviews         Salaries  \\\n",
      "0          TCS    3.8  74.1k Reviews  848.8k Salaries   \n",
      "1    Accenture    4.0    47k Reviews  580.5k Salaries   \n",
      "2    Cognizant    3.9  42.3k Reviews    556k Salaries   \n",
      "3        Wipro    3.8  39.8k Reviews  423.1k Salaries   \n",
      "4    Capgemini    3.9  34.4k Reviews    413k Salaries   \n",
      "\n",
      "                                          Interviews        Jobs  \n",
      "0  https://www.ambitionbox.com/interviews/tcs-int...    926 Jobs  \n",
      "1  https://www.ambitionbox.com/interviews/accentu...  30.3k Jobs  \n",
      "2  https://www.ambitionbox.com/interviews/cogniza...    390 Jobs  \n",
      "3  https://www.ambitionbox.com/interviews/wipro-i...    232 Jobs  \n",
      "4  https://www.ambitionbox.com/interviews/capgemi...    707 Jobs  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "# Define headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize lists to store data\n",
    "company_names = []\n",
    "ratings = []\n",
    "reviews = []\n",
    "salaries = []\n",
    "interviews = []\n",
    "jobs = []\n",
    "\n",
    "# Iterate through 503 pages\n",
    "for page_num in range(1, 504):\n",
    "    # Define the URL for each page\n",
    "    url = f'https://www.ambitionbox.com/list-of-companies?campaign=desktop_nav&page={page_num}'\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all company cards\n",
    "        company_cards = soup.find_all('div', class_='companyCardWrapper')\n",
    "        \n",
    "        # Loop through each company card and extract information\n",
    "        for card in company_cards:\n",
    "            try:\n",
    "                # Extract company name\n",
    "                company_name_elem = card.find('h2', class_='companyCardWrapper__companyName')\n",
    "                company_name = company_name_elem.text.strip() if company_name_elem else 'N/A'\n",
    "                company_names.append(company_name)\n",
    "\n",
    "                # Extract company rating\n",
    "                company_rating_elem = card.find('span', class_='companyCardWrapper__companyRatingValue')\n",
    "                company_rating = company_rating_elem.text.strip() if company_rating_elem else 'N/A'\n",
    "                ratings.append(company_rating)\n",
    "\n",
    "                # Extract number of reviews\n",
    "                reviews_count_elem = card.find('a', href=lambda href: href and '/reviews/' in href)\n",
    "                reviews_count = reviews_count_elem.text.strip() if reviews_count_elem else 'N/A'\n",
    "                reviews.append(reviews_count)\n",
    "\n",
    "                # Extract number of salaries\n",
    "                salaries_count_elem = card.find('a', href=lambda href: href and '/salaries/' in href)\n",
    "                salaries_count = salaries_count_elem.text.strip() if salaries_count_elem else 'N/A'\n",
    "                salaries.append(salaries_count)\n",
    "\n",
    "                # Extract number of interviews\n",
    "                company_url = card.find('a', href=lambda href: href and '/overview/' in href)['href']\n",
    "                interview_url = urllib.parse.urljoin('https://www.ambitionbox.com', company_url.replace('overview', 'interviews'))\n",
    "                interviews.append(interview_url)\n",
    "\n",
    "                # Extract number of jobs\n",
    "                jobs_count_elem = card.find('a', href=lambda href: href and '/jobs/' in href)\n",
    "                jobs_count = jobs_count_elem.text.strip() if jobs_count_elem else 'N/A'\n",
    "                jobs.append(jobs_count)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred for company on page {page_num}: {e}\")\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Company Name': company_names,\n",
    "    'Rating': ratings,\n",
    "    'Reviews': reviews,\n",
    "    'Salaries': salaries,\n",
    "    'Interviews': interviews,\n",
    "    'Jobs': jobs\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Salaries</th>\n",
       "      <th>Interviews</th>\n",
       "      <th>Jobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCS</td>\n",
       "      <td>3.8</td>\n",
       "      <td>74.1k Reviews</td>\n",
       "      <td>848.8k Salaries</td>\n",
       "      <td>https://www.ambitionbox.com/interviews/tcs-int...</td>\n",
       "      <td>926 Jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>4.0</td>\n",
       "      <td>47k Reviews</td>\n",
       "      <td>580.5k Salaries</td>\n",
       "      <td>https://www.ambitionbox.com/interviews/accentu...</td>\n",
       "      <td>30.3k Jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cognizant</td>\n",
       "      <td>3.9</td>\n",
       "      <td>42.3k Reviews</td>\n",
       "      <td>556k Salaries</td>\n",
       "      <td>https://www.ambitionbox.com/interviews/cogniza...</td>\n",
       "      <td>390 Jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>3.8</td>\n",
       "      <td>39.8k Reviews</td>\n",
       "      <td>423.1k Salaries</td>\n",
       "      <td>https://www.ambitionbox.com/interviews/wipro-i...</td>\n",
       "      <td>232 Jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capgemini</td>\n",
       "      <td>3.9</td>\n",
       "      <td>34.4k Reviews</td>\n",
       "      <td>413k Salaries</td>\n",
       "      <td>https://www.ambitionbox.com/interviews/capgemi...</td>\n",
       "      <td>707 Jobs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company Name Rating        Reviews         Salaries  \\\n",
       "0          TCS    3.8  74.1k Reviews  848.8k Salaries   \n",
       "1    Accenture    4.0    47k Reviews  580.5k Salaries   \n",
       "2    Cognizant    3.9  42.3k Reviews    556k Salaries   \n",
       "3        Wipro    3.8  39.8k Reviews  423.1k Salaries   \n",
       "4    Capgemini    3.9  34.4k Reviews    413k Salaries   \n",
       "\n",
       "                                          Interviews        Jobs  \n",
       "0  https://www.ambitionbox.com/interviews/tcs-int...    926 Jobs  \n",
       "1  https://www.ambitionbox.com/interviews/accentu...  30.3k Jobs  \n",
       "2  https://www.ambitionbox.com/interviews/cogniza...    390 Jobs  \n",
       "3  https://www.ambitionbox.com/interviews/wipro-i...    232 Jobs  \n",
       "4  https://www.ambitionbox.com/interviews/capgemi...    707 Jobs  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('job_profile.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
